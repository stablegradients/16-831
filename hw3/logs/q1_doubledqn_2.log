/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/envs/registration.py:440: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/core.py:268: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  deprecation(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  logger.deprecation(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(done, (bool, np.bool8)):
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `step()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")



LOGGING TO:  /zfsauton2/home/shrinivr/16831-S25-HW/hw3/rob831/scripts/../../data/q1_doubledqn_2_LunarLander-v3_13-03-2025_16-10-31 



########################
logging outputs to  /zfsauton2/home/shrinivr/16831-S25-HW/hw3/rob831/scripts/../../data/q1_doubledqn_2_LunarLander-v3_13-03-2025_16-10-31
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.000711
/zfsauton2/home/shrinivr/.local/lib/python3.10/site-packages/tensorboardX/summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  scalar = float(scalar)
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0007112026214599609
Done logging...




********** Iteration 1000 ************

Training agent...


********** Iteration 2000 ************

Training agent...


********** Iteration 3000 ************

Training agent...


********** Iteration 4000 ************

Training agent...


********** Iteration 5000 ************

Training agent...


********** Iteration 6000 ************

Training agent...


********** Iteration 7000 ************

Training agent...


********** Iteration 8000 ************

Training agent...


********** Iteration 9000 ************

Training agent...


********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -251.497227
best mean reward -inf
running time 22.557026
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -251.49722743704456
TimeSinceStart : 22.557026386260986
Training Loss : 1.3558309078216553
Done logging...




********** Iteration 11000 ************

Training agent...


********** Iteration 12000 ************

Training agent...


********** Iteration 13000 ************

Training agent...


********** Iteration 14000 ************

Training agent...


********** Iteration 15000 ************

Training agent...


********** Iteration 16000 ************

Training agent...


********** Iteration 17000 ************

Training agent...


********** Iteration 18000 ************

Training agent...


********** Iteration 19000 ************

Training agent...


********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -237.033498
best mean reward -inf
running time 54.828369
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -237.03349752554274
TimeSinceStart : 54.82836866378784
Training Loss : 2.3803064823150635
Done logging...




********** Iteration 21000 ************

Training agent...


********** Iteration 22000 ************

Training agent...


********** Iteration 23000 ************

Training agent...


********** Iteration 24000 ************

Training agent...


********** Iteration 25000 ************

Training agent...


********** Iteration 26000 ************

Training agent...


********** Iteration 27000 ************

Training agent...


********** Iteration 28000 ************

Training agent...


********** Iteration 29000 ************

Training agent...


********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -227.769064
best mean reward -inf
running time 92.692970
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -227.76906431043233
TimeSinceStart : 92.69296979904175
Training Loss : 1.7227755784988403
Done logging...




********** Iteration 31000 ************

Training agent...


********** Iteration 32000 ************

Training agent...


********** Iteration 33000 ************

Training agent...


********** Iteration 34000 ************

Training agent...


********** Iteration 35000 ************

Training agent...


********** Iteration 36000 ************

Training agent...


********** Iteration 37000 ************

Training agent...


********** Iteration 38000 ************

Training agent...


********** Iteration 39000 ************

Training agent...


********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -212.183144
best mean reward -212.183144
running time 132.404973
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -212.1831436537577
Train_BestReturn : -212.1831436537577
TimeSinceStart : 132.40497303009033
Training Loss : 0.4653382897377014
Done logging...




********** Iteration 41000 ************

Training agent...


********** Iteration 42000 ************

Training agent...


********** Iteration 43000 ************

Training agent...


********** Iteration 44000 ************

Training agent...


********** Iteration 45000 ************

Training agent...


********** Iteration 46000 ************

Training agent...


********** Iteration 47000 ************

Training agent...


********** Iteration 48000 ************

Training agent...


********** Iteration 49000 ************

Training agent...


********** Iteration 50000 ************

Training agent...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -192.520190
best mean reward -192.520190
running time 168.110039
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -192.5201897713109
Train_BestReturn : -192.5201897713109
TimeSinceStart : 168.11003851890564
Training Loss : 0.2016376554965973
Done logging...




********** Iteration 51000 ************

Training agent...


********** Iteration 52000 ************

Training agent...


********** Iteration 53000 ************

Training agent...


********** Iteration 54000 ************

Training agent...


********** Iteration 55000 ************

Training agent...


********** Iteration 56000 ************

Training agent...


********** Iteration 57000 ************

Training agent...


********** Iteration 58000 ************

Training agent...


********** Iteration 59000 ************

Training agent...


********** Iteration 60000 ************

Training agent...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -177.023364
best mean reward -177.023364
running time 204.350125
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -177.02336419839517
Train_BestReturn : -177.02336419839517
TimeSinceStart : 204.35012531280518
Training Loss : 0.3225332796573639
Done logging...




********** Iteration 61000 ************

Training agent...


********** Iteration 62000 ************

Training agent...


********** Iteration 63000 ************

Training agent...


********** Iteration 64000 ************

Training agent...


********** Iteration 65000 ************

Training agent...


********** Iteration 66000 ************

Training agent...


********** Iteration 67000 ************

Training agent...


********** Iteration 68000 ************

Training agent...


********** Iteration 69000 ************

Training agent...


********** Iteration 70000 ************

Training agent...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -168.564295
best mean reward -168.564295
running time 240.484743
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -168.5642954920452
Train_BestReturn : -168.5642954920452
TimeSinceStart : 240.48474264144897
Training Loss : 0.26389846205711365
Done logging...




********** Iteration 71000 ************

Training agent...


********** Iteration 72000 ************

Training agent...


********** Iteration 73000 ************

Training agent...


********** Iteration 74000 ************

Training agent...


********** Iteration 75000 ************

Training agent...


********** Iteration 76000 ************

Training agent...


********** Iteration 77000 ************

Training agent...


********** Iteration 78000 ************

Training agent...


********** Iteration 79000 ************

Training agent...


********** Iteration 80000 ************

Training agent...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -145.832130
best mean reward -145.832130
running time 277.391668
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -145.83213042508694
Train_BestReturn : -145.83213042508694
TimeSinceStart : 277.39166808128357
Training Loss : 0.2972458600997925
Done logging...




********** Iteration 81000 ************

Training agent...


********** Iteration 82000 ************

Training agent...


********** Iteration 83000 ************

Training agent...


********** Iteration 84000 ************

Training agent...


********** Iteration 85000 ************

Training agent...


********** Iteration 86000 ************

Training agent...


********** Iteration 87000 ************

Training agent...


********** Iteration 88000 ************

Training agent...


********** Iteration 89000 ************

Training agent...


********** Iteration 90000 ************

Training agent...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -118.093625
best mean reward -118.093625
running time 312.953520
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -118.09362456748505
Train_BestReturn : -118.09362456748505
TimeSinceStart : 312.9535195827484
Training Loss : 0.27970725297927856
Done logging...




********** Iteration 91000 ************

Training agent...


********** Iteration 92000 ************

Training agent...


********** Iteration 93000 ************

Training agent...


********** Iteration 94000 ************

Training agent...


********** Iteration 95000 ************

Training agent...


********** Iteration 96000 ************

Training agent...


********** Iteration 97000 ************

Training agent...


********** Iteration 98000 ************

Training agent...


********** Iteration 99000 ************

Training agent...


********** Iteration 100000 ************

Training agent...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -97.682367
best mean reward -97.682367
running time 351.946775
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -97.68236664285924
Train_BestReturn : -97.68236664285924
TimeSinceStart : 351.9467751979828
Training Loss : 1.168153166770935
Done logging...




********** Iteration 101000 ************

Training agent...


********** Iteration 102000 ************

Training agent...


********** Iteration 103000 ************

Training agent...


********** Iteration 104000 ************

Training agent...


********** Iteration 105000 ************

Training agent...


********** Iteration 106000 ************

Training agent...


********** Iteration 107000 ************

Training agent...


********** Iteration 108000 ************

Training agent...


********** Iteration 109000 ************

Training agent...


********** Iteration 110000 ************

Training agent...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -85.109256
best mean reward -85.109256
running time 389.119879
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -85.1092561495484
Train_BestReturn : -85.1092561495484
TimeSinceStart : 389.1198785305023
Training Loss : 0.24628350138664246
Done logging...




********** Iteration 111000 ************

Training agent...


********** Iteration 112000 ************

Training agent...


********** Iteration 113000 ************

Training agent...


********** Iteration 114000 ************

Training agent...


********** Iteration 115000 ************

Training agent...


********** Iteration 116000 ************

Training agent...


********** Iteration 117000 ************

Training agent...


********** Iteration 118000 ************

Training agent...


********** Iteration 119000 ************

Training agent...


********** Iteration 120000 ************

Training agent...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -66.619411
best mean reward -66.619411
running time 424.010435
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -66.6194105591938
Train_BestReturn : -66.6194105591938
TimeSinceStart : 424.01043486595154
Training Loss : 0.23946423828601837
Done logging...




********** Iteration 121000 ************

Training agent...


********** Iteration 122000 ************

Training agent...


********** Iteration 123000 ************

Training agent...


********** Iteration 124000 ************

Training agent...


********** Iteration 125000 ************

Training agent...


********** Iteration 126000 ************

Training agent...


********** Iteration 127000 ************

Training agent...


********** Iteration 128000 ************

Training agent...


********** Iteration 129000 ************

Training agent...


********** Iteration 130000 ************

Training agent...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -35.961517
best mean reward -35.961517
running time 458.497577
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -35.961517083535284
Train_BestReturn : -35.961517083535284
TimeSinceStart : 458.49757742881775
Training Loss : 0.10957063734531403
Done logging...




********** Iteration 131000 ************

Training agent...


********** Iteration 132000 ************

Training agent...


********** Iteration 133000 ************

Training agent...


********** Iteration 134000 ************

Training agent...


********** Iteration 135000 ************

Training agent...


********** Iteration 136000 ************

Training agent...


********** Iteration 137000 ************

Training agent...


********** Iteration 138000 ************

Training agent...


********** Iteration 139000 ************

Training agent...


********** Iteration 140000 ************

Training agent...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -8.683136
best mean reward -8.683136
running time 491.867759
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -8.683135791025038
Train_BestReturn : -8.683135791025038
TimeSinceStart : 491.8677587509155
Training Loss : 0.14240272343158722
Done logging...




********** Iteration 141000 ************

Training agent...


********** Iteration 142000 ************

Training agent...


********** Iteration 143000 ************

Training agent...


********** Iteration 144000 ************

Training agent...


********** Iteration 145000 ************

Training agent...


********** Iteration 146000 ************

Training agent...


********** Iteration 147000 ************

Training agent...


********** Iteration 148000 ************

Training agent...


********** Iteration 149000 ************

Training agent...


********** Iteration 150000 ************

Training agent...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 21.798576
best mean reward 21.798576
running time 525.847514
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 21.79857572813683
Train_BestReturn : 21.79857572813683
TimeSinceStart : 525.8475136756897
Training Loss : 0.1177186667919159
Done logging...




********** Iteration 151000 ************

Training agent...


********** Iteration 152000 ************

Training agent...


********** Iteration 153000 ************

Training agent...


********** Iteration 154000 ************

Training agent...


********** Iteration 155000 ************

Training agent...


********** Iteration 156000 ************

Training agent...


********** Iteration 157000 ************

Training agent...


********** Iteration 158000 ************

Training agent...


********** Iteration 159000 ************

Training agent...


********** Iteration 160000 ************

Training agent...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 39.894861
best mean reward 39.894861
running time 558.221869
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 39.894861490321354
Train_BestReturn : 39.894861490321354
TimeSinceStart : 558.2218685150146
Training Loss : 0.16357356309890747
Done logging...




********** Iteration 161000 ************

Training agent...


********** Iteration 162000 ************

Training agent...


********** Iteration 163000 ************

Training agent...


********** Iteration 164000 ************

Training agent...


********** Iteration 165000 ************

Training agent...


********** Iteration 166000 ************

Training agent...


********** Iteration 167000 ************

Training agent...


********** Iteration 168000 ************

Training agent...


********** Iteration 169000 ************

Training agent...


********** Iteration 170000 ************

Training agent...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 52.228556
best mean reward 52.228556
running time 592.540769
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 52.22855621130471
Train_BestReturn : 52.22855621130471
TimeSinceStart : 592.5407688617706
Training Loss : 0.12417773902416229
Done logging...




********** Iteration 171000 ************

Training agent...


********** Iteration 172000 ************

Training agent...


********** Iteration 173000 ************

Training agent...


********** Iteration 174000 ************

Training agent...


********** Iteration 175000 ************

Training agent...


********** Iteration 176000 ************

Training agent...


********** Iteration 177000 ************

Training agent...


********** Iteration 178000 ************

Training agent...


********** Iteration 179000 ************

Training agent...


********** Iteration 180000 ************

Training agent...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 54.041332
best mean reward 54.041332
running time 630.589694
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 54.04133155094926
Train_BestReturn : 54.04133155094926
TimeSinceStart : 630.5896944999695
Training Loss : 0.3840342164039612
Done logging...




********** Iteration 181000 ************

Training agent...


********** Iteration 182000 ************

Training agent...


********** Iteration 183000 ************

Training agent...


********** Iteration 184000 ************

Training agent...


********** Iteration 185000 ************

Training agent...


********** Iteration 186000 ************

Training agent...


********** Iteration 187000 ************

Training agent...


********** Iteration 188000 ************

Training agent...


********** Iteration 189000 ************

Training agent...


********** Iteration 190000 ************

Training agent...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 64.228438
best mean reward 64.228438
running time 665.698899
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 64.22843758287226
Train_BestReturn : 64.22843758287226
TimeSinceStart : 665.6988987922668
Training Loss : 0.09444666653871536
Done logging...




********** Iteration 191000 ************

Training agent...


********** Iteration 192000 ************

Training agent...


********** Iteration 193000 ************

Training agent...


********** Iteration 194000 ************

Training agent...


********** Iteration 195000 ************

Training agent...


********** Iteration 196000 ************

Training agent...


********** Iteration 197000 ************

Training agent...


********** Iteration 198000 ************

Training agent...


********** Iteration 199000 ************

Training agent...


********** Iteration 200000 ************

Training agent...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 67.667537
best mean reward 67.667537
running time 704.625888
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 67.66753656629635
Train_BestReturn : 67.66753656629635
TimeSinceStart : 704.6258878707886
Training Loss : 0.18255862593650818
Done logging...




********** Iteration 201000 ************

Training agent...


********** Iteration 202000 ************

Training agent...


********** Iteration 203000 ************

Training agent...


********** Iteration 204000 ************

Training agent...


********** Iteration 205000 ************

Training agent...


********** Iteration 206000 ************

Training agent...


********** Iteration 207000 ************

Training agent...


********** Iteration 208000 ************

Training agent...


********** Iteration 209000 ************

Training agent...


********** Iteration 210000 ************

Training agent...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 75.653526
best mean reward 75.653526
running time 740.284954
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 75.65352558080487
Train_BestReturn : 75.65352558080487
TimeSinceStart : 740.2849535942078
Training Loss : 0.168268620967865
Done logging...




********** Iteration 211000 ************

Training agent...


********** Iteration 212000 ************

Training agent...


********** Iteration 213000 ************

Training agent...


********** Iteration 214000 ************

Training agent...


********** Iteration 215000 ************

Training agent...


********** Iteration 216000 ************

Training agent...


********** Iteration 217000 ************

Training agent...


********** Iteration 218000 ************

Training agent...


********** Iteration 219000 ************

Training agent...


********** Iteration 220000 ************

Training agent...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 82.241284
best mean reward 82.241284
running time 771.629960
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 82.24128385087813
Train_BestReturn : 82.24128385087813
TimeSinceStart : 771.629959821701
Training Loss : 0.1922900676727295
Done logging...




********** Iteration 221000 ************

Training agent...


********** Iteration 222000 ************

Training agent...


********** Iteration 223000 ************

Training agent...


********** Iteration 224000 ************

Training agent...


********** Iteration 225000 ************

Training agent...


********** Iteration 226000 ************

Training agent...


********** Iteration 227000 ************

Training agent...


********** Iteration 228000 ************

Training agent...


********** Iteration 229000 ************

Training agent...


********** Iteration 230000 ************

Training agent...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 90.769914
best mean reward 90.769914
running time 804.825144
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 90.76991373083722
Train_BestReturn : 90.76991373083722
TimeSinceStart : 804.8251442909241
Training Loss : 0.2817789614200592
Done logging...




********** Iteration 231000 ************

Training agent...


********** Iteration 232000 ************

Training agent...


********** Iteration 233000 ************

Training agent...


********** Iteration 234000 ************

Training agent...


********** Iteration 235000 ************

Training agent...


********** Iteration 236000 ************

Training agent...


********** Iteration 237000 ************

Training agent...


********** Iteration 238000 ************

Training agent...


********** Iteration 239000 ************

Training agent...


********** Iteration 240000 ************

Training agent...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 99.089897
best mean reward 99.089897
running time 836.914660
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 99.08989712853533
Train_BestReturn : 99.08989712853533
TimeSinceStart : 836.9146602153778
Training Loss : 0.07154014706611633
Done logging...




********** Iteration 241000 ************

Training agent...


********** Iteration 242000 ************

Training agent...


********** Iteration 243000 ************

Training agent...


********** Iteration 244000 ************

Training agent...


********** Iteration 245000 ************

Training agent...


********** Iteration 246000 ************

Training agent...


********** Iteration 247000 ************

Training agent...


********** Iteration 248000 ************

Training agent...


********** Iteration 249000 ************

Training agent...


********** Iteration 250000 ************

Training agent...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 99.216767
best mean reward 99.216767
running time 867.960992
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 99.21676742369898
Train_BestReturn : 99.21676742369898
TimeSinceStart : 867.9609916210175
Training Loss : 0.06099659204483032
Done logging...




********** Iteration 251000 ************

Training agent...


********** Iteration 252000 ************

Training agent...


********** Iteration 253000 ************

Training agent...


********** Iteration 254000 ************

Training agent...


********** Iteration 255000 ************

Training agent...


********** Iteration 256000 ************

Training agent...


********** Iteration 257000 ************

Training agent...


********** Iteration 258000 ************

Training agent...


********** Iteration 259000 ************

Training agent...


********** Iteration 260000 ************

Training agent...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 101.847358
best mean reward 101.847358
running time 899.380135
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 101.84735770812473
Train_BestReturn : 101.84735770812473
TimeSinceStart : 899.3801352977753
Training Loss : 0.1311626136302948
Done logging...




********** Iteration 261000 ************

Training agent...


********** Iteration 262000 ************

Training agent...


********** Iteration 263000 ************

Training agent...


********** Iteration 264000 ************

Training agent...


********** Iteration 265000 ************

Training agent...


********** Iteration 266000 ************

Training agent...


********** Iteration 267000 ************

Training agent...


********** Iteration 268000 ************

Training agent...


********** Iteration 269000 ************

Training agent...


********** Iteration 270000 ************

Training agent...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 103.888774
best mean reward 103.888774
running time 928.987411
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 103.88877405000356
Train_BestReturn : 103.88877405000356
TimeSinceStart : 928.9874107837677
Training Loss : 0.11435041576623917
Done logging...




********** Iteration 271000 ************

Training agent...


********** Iteration 272000 ************

Training agent...


********** Iteration 273000 ************

Training agent...


********** Iteration 274000 ************

Training agent...


********** Iteration 275000 ************

Training agent...


********** Iteration 276000 ************

Training agent...


********** Iteration 277000 ************

Training agent...


********** Iteration 278000 ************

Training agent...


********** Iteration 279000 ************

Training agent...


********** Iteration 280000 ************

Training agent...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 104.199293
best mean reward 104.199293
running time 960.326522
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 104.19929328804223
Train_BestReturn : 104.19929328804223
TimeSinceStart : 960.3265221118927
Training Loss : 0.27909985184669495
Done logging...




********** Iteration 281000 ************

Training agent...


********** Iteration 282000 ************

Training agent...


********** Iteration 283000 ************

Training agent...


********** Iteration 284000 ************

Training agent...


********** Iteration 285000 ************

Training agent...


********** Iteration 286000 ************

Training agent...


********** Iteration 287000 ************

Training agent...


********** Iteration 288000 ************

Training agent...


********** Iteration 289000 ************

Training agent...


********** Iteration 290000 ************

Training agent...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 104.629933
best mean reward 104.629933
running time 989.704368
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 104.62993338795319
Train_BestReturn : 104.62993338795319
TimeSinceStart : 989.7043681144714
Training Loss : 0.1774156391620636
Done logging...




********** Iteration 291000 ************

Training agent...


********** Iteration 292000 ************

Training agent...


********** Iteration 293000 ************

Training agent...


********** Iteration 294000 ************

Training agent...


********** Iteration 295000 ************

Training agent...


********** Iteration 296000 ************

Training agent...


********** Iteration 297000 ************

Training agent...


********** Iteration 298000 ************

Training agent...


********** Iteration 299000 ************

Training agent...


********** Iteration 300000 ************

Training agent...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 95.474344
best mean reward 104.629933
running time 1019.147524
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 95.47434435596247
Train_BestReturn : 104.62993338795319
TimeSinceStart : 1019.1475238800049
Training Loss : 0.16797353327274323
Done logging...


